{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import future annotations for type hints compatibility\n",
    "from __future__ import annotations\n",
    "\n",
    "\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import pinv, svd\n",
    "from scipy.signal import savgol_filter, find_peaks\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import urllib.error\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# Set plotting style using Seaborn\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moisessalgado/Library/Python/3.12/lib/python/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Memory-efficient CSV loader for local paths or URLs\n",
    "def read_large_csv(path: str | Path) -> np.ndarray:\n",
    "    \"\"\"Load CSV from a local file path or GitHub URL with basic validation.\n",
    "    \n",
    "    Args:\n",
    "        path (str | Path): The file path or URL to the CSV file.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: The loaded data as a numpy array.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the dataset has insufficient columns, invalid labels, or non-numeric/NaN labels.\n",
    "        urllib.error.URLError: If the URL request fails.\n",
    "        FileNotFoundError: If the file is not found.\n",
    "        RuntimeError: For other loading errors.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        data = pd.read_csv(path, header=None).to_numpy(dtype=np.float64)\n",
    "        if data.shape[1] < 2:  # Ensure at least one feature and label column\n",
    "            raise ValueError(f\"Dataset {path} has insufficient columns: {data.shape[1]}\")\n",
    "        if len(np.unique(data[:, -1])) < 1:  # Ensure valid labels\n",
    "            raise ValueError(f\"Dataset {path} has no valid labels\")\n",
    "        # Check for non-numeric or NaN values in labels\n",
    "        if not np.issubdtype(data[:, -1].dtype, np.number) or np.any(np.isnan(data[:, -1])):\n",
    "            raise ValueError(f\"Dataset {path} contains non-numeric or NaN labels\")\n",
    "        return data\n",
    "    except urllib.error.URLError as e:\n",
    "        raise ValueError(f\"Failed to load CSV from {path}: {e}\")\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"File not found at {path}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading CSV from {path}: {e}\")\n",
    "\n",
    "# Outlier detection\n",
    "def is_outlier_mean(x: np.ndarray, thresh: float = 3.0) -> np.ndarray:\n",
    "\n",
    "    \"\"\"Detect outliers in a numpy array based on mean and standard deviation.\n",
    "    \n",
    "    Args:\n",
    "        x (np.ndarray): The input array to check for outliers.\n",
    "        thresh (float, optional): Threshold multiplier for standard deviation. Defaults to 3.0.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Boolean array indicating outliers.\n",
    "    \"\"\"\n",
    "    mu, sigma = np.nanmean(x), np.nanstd(x)\n",
    "    return np.abs(x - mu) > thresh * sigma\n",
    "\n",
    "# Custom scorer for AUC-ROC with multiclass support\n",
    "auc_scorer = make_scorer(roc_auc_score, needs_proba=True, multi_class='ovr')\n",
    "\n",
    "# Classification AUC-ROC\n",
    "def classification_auc(\n",
    "    data: np.ndarray,\n",
    "    feature_idx: list[int],\n",
    "    clf_name: str = \"svm\",\n",
    "    folds: int = 5,\n",
    "    seed: int = 42\n",
    ") -> float:\n",
    "    \"\"\"Calculate AUC-ROC score using cross-validation for a given classifier.\n",
    "    \n",
    "    Args:\n",
    "        data (np.ndarray): The full dataset including features and labels.\n",
    "        feature_idx (list[int]): Indices of features to use.\n",
    "        clf_name (str, optional): Name of the classifier. Defaults to \"svm\".\n",
    "        folds (int, optional): Number of cross-validation folds. Defaults to 5.\n",
    "        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n",
    "    \n",
    "    Returns:\n",
    "        float: Mean AUC-ROC score as a percentage.\n",
    "    \"\"\"\n",
    "\n",
    "    X = data[:, feature_idx]\n",
    "    y = data[:, -1]\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    if clf_name == \"svm\":\n",
    "        clf = SVC(kernel=\"rbf\", gamma=\"scale\", C=1.0, probability=True)\n",
    "    elif clf_name == \"rf\":\n",
    "        clf = RandomForestClassifier(n_estimators=30, random_state=seed)\n",
    "    elif clf_name == \"lr\":\n",
    "        clf = LogisticRegression(random_state=seed, max_iter=1000)\n",
    "    elif clf_name == \"dt\":\n",
    "        clf = DecisionTreeClassifier(random_state=seed)\n",
    "    elif clf_name == \"knn\":\n",
    "        clf = KNeighborsClassifier(n_neighbors=5)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown classifier: {clf_name}. Use 'svm', 'rf', 'lr', 'dt', or 'knn'.\")\n",
    "    cv = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "    try:\n",
    "        auc = cross_val_score(clf, X, y, cv=cv, scoring=auc_scorer).mean() * 100.0\n",
    "    except ValueError as e:\n",
    "        print(f\"Error in AUC calculation for {clf_name}: {e}\")\n",
    "        print(f\"X shape: {X.shape}, y unique values: {np.unique(y)}\")\n",
    "        auc = np.nan\n",
    "    return auc\n",
    "\n",
    "# Classification Accuracy\n",
    "def classification_accuracy(\n",
    "    data: np.ndarray,\n",
    "    feature_idx: list[int],\n",
    "    clf_name: str = \"svm\",\n",
    "    folds: int = 5,\n",
    "    seed: int = 42\n",
    ") -> float:\n",
    "    \"\"\"Calculate accuracy score using cross-validation for a given classifier.\n",
    "    \n",
    "    Args:\n",
    "        data (np.ndarray): The full dataset including features and labels.\n",
    "        feature_idx (list[int]): Indices of features to use.\n",
    "        clf_name (str, optional): Name of the classifier. Defaults to \"svm\".\n",
    "        folds (int, optional): Number of cross-validation folds. Defaults to 5.\n",
    "        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n",
    "    \n",
    "    Returns:\n",
    "        float: Mean accuracy score as a percentage.\n",
    "    \"\"\"\n",
    "\n",
    "    X = data[:, feature_idx]\n",
    "    y = data[:, -1]\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    if clf_name == \"svm\":\n",
    "        clf = SVC(kernel=\"rbf\", gamma=\"scale\", C=1.0)\n",
    "    elif clf_name == \"rf\":\n",
    "        clf = RandomForestClassifier(n_estimators=30, random_state=seed)\n",
    "    elif clf_name == \"lr\":\n",
    "        clf = LogisticRegression(random_state=seed, max_iter=1000)\n",
    "    elif clf_name == \"dt\":\n",
    "        clf = DecisionTreeClassifier(random_state=seed)\n",
    "    elif clf_name == \"knn\":\n",
    "        clf = KNeighborsClassifier(n_neighbors=5)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown classifier: {clf_name}. Use 'svm', 'rf', 'lr', 'dt', or 'knn'.\")\n",
    "    cv = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "    try:\n",
    "        acc = cross_val_score(clf, X, y, cv=cv, scoring=\"accuracy\").mean() * 100.0\n",
    "    except ValueError as e:\n",
    "        print(f\"Error in accuracy calculation for {clf_name}: {e}\")\n",
    "        print(f\"X shape: {X.shape}, y unique values: {np.unique(y)}\")\n",
    "        acc = np.nan\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column-wise normalization\n",
    "def normc(A: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Normalize the columns of a numpy array.\n",
    "    \n",
    "    Args:\n",
    "        A (np.ndarray): The input array to normalize.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: The normalized array.\n",
    "    \"\"\"\n",
    "    return normalize(A, axis=0)\n",
    "\n",
    "# Placeholder for Frobenius-penalized DNN\n",
    "def frobenius_dnn(X, y):\n",
    "    # Placeholder: Implement autoencoder-based feature selection (Li, 2023)\n",
    "    \"\"\"Placeholder for Frobenius-penalized Deep Neural Network feature selection.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): Feature matrix.\n",
    "        y (np.ndarray): Target vector.\n",
    "    \n",
    "    Returns:\n",
    "        list: Dummy list of the first 50 feature indices.\n",
    "    \"\"\"\n",
    "\n",
    "    return list(range(min(50, X.shape[1])))  # Dummy: select first 50 features\n",
    "\n",
    "# Placeholder for FREEFORM\n",
    "def freeform_llm(X, y):\n",
    "    # Placeholder: Implement LLM-based feature selection (Lee et al., 2024)\n",
    "    \"\"\"Placeholder for FREEFORM (LLM-based) feature selection.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): Feature matrix.\n",
    "        y (np.ndarray): Target vector.\n",
    "    \n",
    "    Returns:\n",
    "        list: Dummy list of the first 50 feature indices.\n",
    "    \"\"\"\n",
    "\n",
    "    return list(range(min(50, X.shape[1])))  # Dummy: select first 50 features\n",
    "\n",
    "# DRPT feature selection (void function)\n",
    "def run_feature_selection(\n",
    "    dataset: str,\n",
    "    clf_name: str,\n",
    "    base_dir: str = \"https://raw.githubusercontent.com/def-abraham/Projects/refs/heads/main/Project%203\",\n",
    "    clusters: int = 50,\n",
    "    run_iter: int = 10,\n",
    "    t_perturb: int = 50,\n",
    "    seed: int = 0,\n",
    "    return_features: bool = False,\n",
    "    display_results: bool = True\n",
    ") -> None:\n",
    "    \n",
    "    \"\"\"Perform DRPT feature selection and evaluate with a classifier.\n",
    "    \n",
    "    Args:\n",
    "        dataset (str): Name of the dataset file.\n",
    "        clf_name (str): Name of the classifier to use.\n",
    "        base_dir (str, optional): Base directory or URL for dataset files. Defaults to GitHub raw URL.\n",
    "        clusters (int, optional): Maximum number of feature clusters. Defaults to 50.\n",
    "        run_iter (int, optional): Number of iterations/runs. Defaults to 10.\n",
    "        t_perturb (int, optional): Number of perturbations. Defaults to 50.\n",
    "        seed (int, optional): Random seed for reproducibility. Defaults to 0.\n",
    "        return_features (bool, optional): Whether to return selected features. Defaults to False.\n",
    "        display_results (bool, optional): Whether to display results and plots. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "        None or list: Returns selected features if return_features is True, otherwise None.\n",
    "    \"\"\"\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    \n",
    "    # Track time and memory\n",
    "    start_time = time.time()\n",
    "    start_memory = psutil.Process().memory_info().rss / 1024**3  # GB\n",
    "    \n",
    "    # Construct URL for GitHub-hosted dataset\n",
    "    csv_url = f\"{base_dir}/{dataset}\"\n",
    "    print(f\"\\nLoading {dataset} from {csv_url} ...\")\n",
    "    data = read_large_csv(csv_url)\n",
    "    data = rng.permutation(data)\n",
    "    org_data = data.copy()\n",
    "\n",
    "    # Pre-allocate result arrays\n",
    "    features_picked = np.zeros((clusters, run_iter), dtype=int)\n",
    "    max_auc = np.zeros((run_iter, 2))\n",
    "    max_acc = np.zeros((run_iter, 2))\n",
    "    auc_per_k = [[] for _ in range(run_iter)]  # Store AUC for each k\n",
    "    acc_per_k = [[] for _ in range(run_iter)]  # Store accuracy for each k\n",
    "\n",
    "    # Runs\n",
    "    for run in range(run_iter):\n",
    "        # Stratified 70% train/30% test split\n",
    "        labels, counts = np.unique(data[:, -1], return_counts=True)\n",
    "        train_idx = []\n",
    "        for lbl in labels:\n",
    "            idx_lbl = np.where(data[:, -1] == lbl)[0]\n",
    "            rng.shuffle(idx_lbl)\n",
    "            k = int(np.floor(0.7 * len(idx_lbl)))\n",
    "            train_idx.extend(idx_lbl[:k])\n",
    "        rng.shuffle(train_idx)\n",
    "        data_train = data[train_idx, :]\n",
    "\n",
    "        # Variables\n",
    "        A = data_train[:, :-1]\n",
    "        B = data_train[:, -1]\n",
    "        c_all = A.shape[1]\n",
    "\n",
    "        # Normalise columns\n",
    "        A = normc(A)\n",
    "        C = A.copy()\n",
    "\n",
    "        # Irrelevant-feature removal\n",
    "        X = pinv(A) @ B\n",
    "        cleaned_F = np.arange(c_all)\n",
    "\n",
    "        # Iterative outlier elimination\n",
    "        outliers_len = c_all\n",
    "        list_X = np.zeros((5, c_all))\n",
    "        list_X[0, :] = X\n",
    "        ii = 0\n",
    "        while outliers_len > (c_all * 0.021):\n",
    "            mask = is_outlier_mean(np.abs(list_X[ii, :outliers_len]))\n",
    "            tmp = np.where(mask)[0]\n",
    "            outliers_len = len(tmp)\n",
    "            if outliers_len == 0 or ii == 4:\n",
    "                break\n",
    "            ii += 1\n",
    "            list_X[ii, :outliers_len] = list_X[ii - 1, tmp]\n",
    "        if outliers_len < 10:\n",
    "            outliers_len = is_outlier_mean(np.abs(X)).sum()\n",
    "\n",
    "        # Threshold based on local maxima\n",
    "        peaks, _ = find_peaks(np.abs(X))\n",
    "        threshold = np.mean(np.abs(X)[peaks]) if len(peaks) > 0 else np.mean(np.abs(X))\n",
    "\n",
    "        # Iteratively reduce features\n",
    "        while len(cleaned_F) > (outliers_len * (2 / max(ii, 1))):\n",
    "            irr_F = np.where(np.abs(X) < threshold)[0]\n",
    "            threshold *= 1.03\n",
    "            cleaned_F = np.setxor1d(np.arange(c_all), irr_F, assume_unique=True)\n",
    "\n",
    "        A = A[:, cleaned_F]\n",
    "        C = C[:, cleaned_F]\n",
    "        c_all = len(cleaned_F)\n",
    "\n",
    "        # Perturbation matrix\n",
    "        singular_vals = svd(A, compute_uv=False)\n",
    "        smallest_A = singular_vals.min()\n",
    "        X = pinv(A) @ B\n",
    "        m_error = 10 ** -3 * smallest_A\n",
    "\n",
    "        def _single_perturb(_):\n",
    "            per_val = m_error * rng.random(A.shape)\n",
    "            pA = A + per_val\n",
    "            DX = np.abs(pinv(pA) @ B - X)\n",
    "            return DX\n",
    "\n",
    "        px = joblib.Parallel(n_jobs=-1)(\n",
    "            joblib.delayed(_single_perturb)(i) for i in range(t_perturb)\n",
    "        )\n",
    "        pX = np.mean(px, axis=0)\n",
    "        ent = -np.nansum(C * np.log(C + 1e-12), axis=0).real\n",
    "\n",
    "        # Rank features\n",
    "        pX_smoothed = savgol_filter(pX, 11, polyorder=3)\n",
    "        rounded_pX = pX_smoothed.copy()\n",
    "        for round_metric in range(20, -1, -1):\n",
    "            rounded_pX = np.round(pX_smoothed, round_metric)\n",
    "            if len(np.unique(rounded_pX)) <= 50:\n",
    "                break\n",
    "\n",
    "        selected = []\n",
    "        for key in np.unique(rounded_pX):\n",
    "            idx_px = np.where(rounded_pX == key)[0]\n",
    "            filtered_ent = ent[idx_px]\n",
    "            rounded_ent = filtered_ent.copy()\n",
    "            for round_metric in range(5, -1, -1):\n",
    "                rounded_ent = np.round(filtered_ent, round_metric)\n",
    "                if len(np.unique(rounded_ent)) <= 20:\n",
    "                    break\n",
    "            for ent_val in np.unique(rounded_ent):\n",
    "                idx_ent = idx_px[np.where(rounded_ent == ent_val)[0]]\n",
    "                idx_sorted = idx_ent[np.argsort(np.abs(X[idx_ent]))[::-1]]\n",
    "                selected.append(idx_sorted[0])\n",
    "\n",
    "        # Final ranking with entropy and |x_i|\n",
    "        ent_normalized = (ent - ent.min()) / (ent.max() - ent.min() + 1e-12)\n",
    "        x_abs_normalized = (np.abs(X) - np.abs(X).min()) / (np.abs(X).max() - np.abs(X).min() + 1e-12)\n",
    "        scores = 0.5 * ent_normalized + 0.5 * x_abs_normalized\n",
    "        ranked_features = cleaned_F[np.argsort(scores[selected])[::-1]]\n",
    "        upper_band = min(clusters, len(ranked_features))\n",
    "\n",
    "        # Classify with incremental subsets\n",
    "        best_auc, best_k_auc = 0.0, 0\n",
    "        best_acc, best_k_acc = 0.0, 0\n",
    "        for k in range(1, upper_band + 1):\n",
    "            centres = ranked_features[:k]\n",
    "            auc = classification_auc(org_data, centres.tolist(), clf_name)\n",
    "            acc = classification_accuracy(org_data, centres.tolist(), clf_name)\n",
    "            auc_per_k[run].append((k, auc))\n",
    "            acc_per_k[run].append((k, acc))\n",
    "            if auc > best_auc:\n",
    "                best_auc, best_k_auc = auc, k\n",
    "                features_picked[:k, run] = centres\n",
    "            if acc > best_acc:\n",
    "                best_acc, best_k_acc = acc, k\n",
    "            if best_auc == 100.0 and best_acc == 100.0:\n",
    "                break\n",
    "\n",
    "        max_auc[run, :] = (best_auc, best_k_auc)\n",
    "        max_acc[run, :] = (best_acc, best_k_acc)\n",
    "        if display_results:\n",
    "            print(f\"Run: {run+1}, Selected Features = {best_k_auc:2d}, AUC-ROC = {best_auc:5.2f}%, Accuracy = {best_acc:5.2f}%\")\n",
    "\n",
    "    # Summary (calculated once, outside per-run display)\n",
    "    ave_auc = max_auc[:, 0].mean()\n",
    "    ave_f_auc = max_auc[:, 1].mean()\n",
    "    ave_acc = max_acc[:, 0].mean()\n",
    "    ave_f_acc = max_acc[:, 1].mean()\n",
    "    best_run = int(np.argmax(max_auc[:, 0]))\n",
    "    full_auc = classification_auc(org_data, list(range(org_data.shape[1]-1)), clf_name)\n",
    "    full_acc = classification_accuracy(org_data, list(range(org_data.shape[1]-1)), clf_name)\n",
    "    print(\"\\n------------------------------------------------------------------\")\n",
    "    print(f\"\\nSelected Features (mean) = {ave_f_auc:.2f}\")\n",
    "    print(f\"\\nAUC-ROC (mean) = {ave_auc:.2f}\")\n",
    "    print(f\"\\nAUC-ROC (original) = {full_auc:.2f}\")\n",
    "    print(f\"\\nSelected Features (mean, Accuracy) = {ave_f_acc:.2f}\")\n",
    "    print(f\"\\nAccuracy (mean) = {ave_acc:.2f}\")\n",
    "    print(f\"\\nAccuracy (original) = {full_acc:.2f}\")\n",
    "    print(f\"\\nStandard Deviation of Selected Features (AUC) = {np.std(max_auc[:,1]):.2f}\")\n",
    "    print(f\"\\nStandard Deviation of AUC-ROC = {np.std(max_auc[:,0]):.2f}\")\n",
    "    print(f\"\\nStandard Deviation of Selected Features (Accuracy) = {np.std(max_acc[:,1]):.2f}\")\n",
    "    print(f\"\\nStandard Deviation of Accuracy = {np.std(max_acc[:,0]):.2f}\")\n",
    "    print(\"\\nOptimal subset =\", features_picked[:, best_run][features_picked[:, best_run] > 0])\n",
    "\n",
    "    # Performance metrics\n",
    "    end_time = time.time()\n",
    "    end_memory = psutil.Process().memory_info().rss / 1024**3\n",
    "    running_time = end_time - start_time\n",
    "    memory_usage = end_memory - start_memory\n",
    "    print(f\"\\nRunning Time: {running_time:.2f} seconds\")\n",
    "    print(f\"Memory Usage: {memory_usage:.2f} GB\")\n",
    "\n",
    "    # Plot 1: AUC-ROC vs. Number of Features\n",
    "    if display_results:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        max_k = max(len(run_auc) for run_auc in auc_per_k)\n",
    "        mean_auc = np.full(max_k, np.nan)\n",
    "        std_auc = np.full(max_k, np.nan)\n",
    "        for k in range(max_k):\n",
    "            aucs = [run_auc[k][1] for run_auc in auc_per_k if k < len(run_auc)]\n",
    "            if aucs:\n",
    "                mean_auc[k] = np.mean(aucs)\n",
    "                std_auc[k] = np.std(aucs)\n",
    "        k_values = range(1, max_k + 1)\n",
    "        plt.plot(k_values, mean_auc, label='Mean AUC-ROC', color='blue', marker='o')\n",
    "        plt.fill_between(k_values, mean_auc - std_auc, mean_auc + std_auc, alpha=0.2, color='blue', label='±1 Std')\n",
    "        plt.axhline(y=full_auc, color='red', linestyle='--', label='All Features AUC-ROC')\n",
    "        plt.xlabel('Number of Features')\n",
    "        plt.ylabel('AUC-ROC (%)')\n",
    "        plt.title(f'AUC-ROC vs. Number of Features (DRPT on {dataset})')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Plot 2: Accuracy vs. Number of Features\n",
    "    if display_results:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        mean_acc = np.full(max_k, np.nan)\n",
    "        std_acc = np.full(max_k, np.nan)\n",
    "        for k in range(max_k):\n",
    "            accs = [run_acc[k][1] for run_acc in acc_per_k if k < len(run_acc)]\n",
    "            if accs:\n",
    "                mean_acc[k] = np.mean(accs)\n",
    "                std_acc[k] = np.std(accs)\n",
    "        plt.plot(k_values, mean_acc, label='Mean Accuracy', color='green', marker='o')\n",
    "        plt.fill_between(k_values, mean_acc - std_acc, mean_acc + std_acc, alpha=0.2, color='green', label='±1 Std')\n",
    "        plt.axhline(y=full_acc, color='red', linestyle='--', label='All Features Accuracy')\n",
    "        plt.xlabel('Number of Features')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.title(f'Accuracy vs. Number of Features (DRPT on {dataset})')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Plot 3: Feature Selection Frequency\n",
    "    if display_results:\n",
    "        all_features = features_picked[features_picked > 0]\n",
    "        if len(all_features) > 0:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            unique, counts = np.unique(all_features, return_counts=True)\n",
    "            sns.barplot(x=unique, y=counts, color='skyblue')\n",
    "            plt.xlabel('Feature Index')\n",
    "            plt.ylabel('Selection Frequency')\n",
    "            plt.title(f'Feature Selection Frequency Across {run_iter} Runs')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    # Plot 4: AUC-ROC Distribution Across Runs\n",
    "    if display_results:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.boxplot(data=max_auc[:, 0], color='lightgreen', width=0.4)\n",
    "        plt.axhline(y=full_auc, color='red', linestyle='--', label='All Features AUC-ROC')\n",
    "        plt.ylabel('AUC-ROC (%)')\n",
    "        plt.title(f'AUC-ROC Distribution Across Runs (DRPT)')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Plot 5: Accuracy Distribution Across Runs\n",
    "    if display_results:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.boxplot(data=max_acc[:, 0], color='lightblue', width=0.4)\n",
    "        plt.axhline(y=full_acc, color='red', linestyle='--', label='All Features Accuracy')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.title(f'Accuracy Distribution Across Runs (DRPT)')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Plot 6: Feature Importance Stability Scatter Plot\n",
    "    if display_results:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        feature_presence = np.zeros((run_iter, org_data.shape[1] - 1))\n",
    "        for run in range(run_iter):\n",
    "            selected = features_picked[:, run][features_picked[:, run] > 0]\n",
    "            feature_presence[run, selected] = 1\n",
    "        # Find coordinates of selected features\n",
    "        selected_features_x = []\n",
    "        selected_features_y = []\n",
    "        for run in range(run_iter):\n",
    "            for feature in range(org_data.shape[1] - 1):\n",
    "                if feature_presence[run, feature] == 1:\n",
    "                    selected_features_x.append(feature)\n",
    "                    selected_features_y.append(run)\n",
    "        plt.scatter(selected_features_x, selected_features_y, color='blue', s=50, alpha=0.6)\n",
    "        plt.xlabel('Feature Index')\n",
    "        plt.ylabel('Run')\n",
    "        plt.title(f'Feature Selection Stability Across {run_iter} Runs')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Plot 7: Δx vs. Smoothed Δx\n",
    "    if display_results:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(pX, label='Δx', color='orange')\n",
    "        plt.plot(pX_smoothed, label='Smoothed Δx', color='blue')\n",
    "        plt.xlabel('Feature Index')\n",
    "        plt.ylabel('Δx Value')\n",
    "        plt.title(f'Δx vs. Smoothed Δx (DRPT on {dataset})')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Plot 8: Clustering of Smoothed Δx\n",
    "    if display_results:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.step(range(len(np.unique(rounded_pX))), np.unique(rounded_pX), label='Sorted Smoothed Δx', color='blue')\n",
    "        plt.xlabel('Cluster Index')\n",
    "        plt.ylabel('Smoothed Δx Value')\n",
    "        plt.title(f'Clustering of Smoothed Δx (DRPT on {dataset})')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Return features only if requested\n",
    "    if return_features:\n",
    "        return features_picked[:, best_run][features_picked[:, best_run] > 0].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0378ec40b49945309016056cab335871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Classifier:', options=('svm', 'rf', 'dt', 'knn', 'lr'), value='svm')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8992bd4c426b432baf298c002943e372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Method:', options=('drpt', 'dnn', 'freeform'), value='drpt')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e0a79fe8e64d43bcf8b5510070a985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Dataset:', options=('GDS1615_full_NoFeature.csv', 'GDS968_full_NoFeature.csv', 'GDS531_f…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de046620720847ffbb470babd4f356c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Run Analysis', style=ButtonStyle(), tooltip='Click to run the anal…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1860ca0c70b440e9bd19b551cef3b005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Benchmarking wrapper function\n",
    "def benchmark_methods(dataset: str, methods: list[str], clf_name: str):\n",
    "    \"\"\"Benchmark multiple feature selection methods on a dataset with a single classifier.\n",
    "    \n",
    "    Args:\n",
    "        dataset (str): Name of the dataset file.\n",
    "        methods (list[str]): List of methods to benchmark.\n",
    "        clf_name (str): Name of the classifier to use.\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    start_memory = psutil.Process().memory_info().rss / 1024**3\n",
    "    print(f\"\\nBenchmarking on {dataset} using classifier {clf_name.upper()}\")\n",
    "\n",
    "    # Load and preprocess data\n",
    "    csv_url = f\"https://raw.githubusercontent.com/def-abraham/Projects/refs/heads/main/Project%203/{dataset}\"\n",
    "    try:\n",
    "        data = read_large_csv(csv_url)\n",
    "        data = np.random.default_rng(seed=0).permutation(data)\n",
    "        X_full = data[:, :-1]\n",
    "        y = data[:, -1]\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load dataset {dataset}: {e}\")\n",
    "        return\n",
    "\n",
    "    for method in methods:\n",
    "        print(f\"\\n--- Running {method.upper()} ---\")\n",
    "        if method == \"drpt\":\n",
    "            selected_features = run_feature_selection(dataset, clf_name, return_features=True, display_results=True)\n",
    "        elif method == \"dnn\":\n",
    "            selected_features = frobenius_dnn(X_full, y)\n",
    "        elif method == \"freeform\":\n",
    "            selected_features = freeform_llm(X_full, y)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "        # Evaluate with selected classifier\n",
    "        auc_scores = []\n",
    "        acc_scores = []\n",
    "        for k in range(1, min(50, len(selected_features)) + 1):\n",
    "            X_subset = X_full[:, selected_features[:k]]\n",
    "            auc = classification_auc(data, selected_features[:k], clf_name)\n",
    "            acc = classification_accuracy(data, selected_features[:k], clf_name)\n",
    "            auc_scores.append(auc)\n",
    "            acc_scores.append(acc)\n",
    "        print(f\"{method.upper()} with {clf_name.upper()}: Mean AUC-ROC = {np.mean(auc_scores):.2f}, Std AUC = {np.std(auc_scores):.2f}\")\n",
    "        print(f\"{method.upper()} with {clf_name.upper()}: Mean Accuracy = {np.mean(acc_scores):.2f}, Std Acc = {np.std(acc_scores):.2f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    end_memory = psutil.Process().memory_info().rss / 1024**3\n",
    "    print(f\"\\nBenchmark Time: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Memory Usage: {end_memory - start_memory:.2f} GB\")\n",
    "\n",
    "# Interactive widgets for Jupyter\n",
    "classifiers = [\"svm\", \"rf\", \"dt\", \"knn\", \"lr\"]\n",
    "methods = [\"drpt\", \"dnn\", \"freeform\"]\n",
    "datasets = [\n",
    "    \"GDS1615_full_NoFeature.csv\",\n",
    "    \"GDS968_full_NoFeature.csv\",\n",
    "    \"GDS531_full_NoFeature.csv\",\n",
    "]\n",
    "\n",
    "classifier_dropdown = widgets.Dropdown(\n",
    "    options=classifiers,\n",
    "    value=classifiers[0],\n",
    "    description='Classifier:',\n",
    ")\n",
    "method_dropdown = widgets.Dropdown(\n",
    "    options=methods,\n",
    "    value=methods[0],\n",
    "    description='Method:',\n",
    ")\n",
    "dataset_dropdown = widgets.Dropdown(\n",
    "    options=datasets,\n",
    "    value=datasets[0],\n",
    "    description='Dataset:',\n",
    ")\n",
    "run_button = widgets.Button(\n",
    "    description='Run Analysis',\n",
    "    button_style='success',\n",
    "    tooltip='Click to run the analysis',\n",
    ")\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_run_button_clicked(b):\n",
    "    \"\"\"Callback function to run the analysis when the button is clicked.\n",
    "    \n",
    "    Args:\n",
    "        b: The button widget object.\n",
    "    \"\"\"\n",
    "    \n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        clf_name = classifier_dropdown.value\n",
    "        method = method_dropdown.value\n",
    "        dataset = dataset_dropdown.value\n",
    "        print(f\"Running {method.upper()} on {dataset} using classifier {clf_name.upper()}\")\n",
    "        benchmark_methods(dataset, [method], clf_name)\n",
    "\n",
    "run_button.on_click(on_run_button_clicked)\n",
    "display(classifier_dropdown)\n",
    "display(method_dropdown)\n",
    "display(dataset_dropdown)\n",
    "display(run_button)\n",
    "display(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
